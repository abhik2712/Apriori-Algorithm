{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rating_Predict-Amazon.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPsNAqRGxOZP",
        "outputId": "be99d305-c1cc-44a6-8fc6-a413e94f181c"
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FclRRM21xiP8"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "keras = tf.keras"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe3-7qZExnSK"
      },
      "source": [
        "%matplotlib inline\n",
        "import re\n",
        "import seaborn as sbn\n",
        "import nltk\n",
        "import tqdm as tqdm\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words(\"english\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from math import floor,ceil\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "english_stemmer=nltk.stem.SnowballStemmer('english')\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
        "from keras.layers import LSTM, Dense, Embedding\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "from gensim import summarization\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpmSXERNxrXZ"
      },
      "source": [
        "df = pd.read_csv('Reviews1.csv')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "7aCZVSw2xyPj",
        "outputId": "03662cb8-67e8-4605-fca7-1c201ea39a41"
      },
      "source": [
        "review_data = df[['UserId', 'ProductId', 'Score','Summary','Text']]\n",
        "review_data.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>Score</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A3SGXH7AUHU8GW</td>\n",
              "      <td>B001E4KFG0</td>\n",
              "      <td>5</td>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A1D87F6ZCVE5NK</td>\n",
              "      <td>B00813GRG4</td>\n",
              "      <td>1</td>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ABXLMWJIXXAIN</td>\n",
              "      <td>B000LQOCH0</td>\n",
              "      <td>4</td>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A395BORC6FGVXV</td>\n",
              "      <td>B000UA0QIQ</td>\n",
              "      <td>2</td>\n",
              "      <td>Cough Medicine</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A1UQRSCLF8GW1T</td>\n",
              "      <td>B006K2ZZ7K</td>\n",
              "      <td>5</td>\n",
              "      <td>Great taffy</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           UserId  ...                                               Text\n",
              "0  A3SGXH7AUHU8GW  ...  I have bought several of the Vitality canned d...\n",
              "1  A1D87F6ZCVE5NK  ...  Product arrived labeled as Jumbo Salted Peanut...\n",
              "2   ABXLMWJIXXAIN  ...  This is a confection that has been around a fe...\n",
              "3  A395BORC6FGVXV  ...  If you are looking for the secret ingredient i...\n",
              "4  A1UQRSCLF8GW1T  ...  Great taffy at a great price.  There was a wid...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biqnLYWvy0l8",
        "outputId": "a44dda29-4d35-40e2-d06f-5a5f224940ad"
      },
      "source": [
        "review_data['UserId'].value_counts()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "A3NHUQ33CFH3VM    5\n",
              "A3PJZ8TU8FDQ1K    5\n",
              "A2NLZ3M0OJV9NX    4\n",
              "A3RMGIKUWGPZOK    4\n",
              "A31N6KB160O508    3\n",
              "                 ..\n",
              "A19SDJ2TSSL8HI    1\n",
              "A15SCIHLW004UA    1\n",
              "A1QCP3UXNAN8FJ    1\n",
              "A3RXAU2N8KV45G    1\n",
              "A3NH41GQPFMGQS    1\n",
              "Name: UserId, Length: 2890, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMfIt9Sbx1Mp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab1f67d6-24e6-4742-fab7-9faba72717be"
      },
      "source": [
        "\n",
        "# user1_data = review_data.loc[review_data['UserId'] == 'A3NHUQ33CFH3VM']\n",
        "labels = review_data['Score'].map(lambda x : 1 if int(x) > 3 else 0)\n",
        "print(np.array(labels))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0 1 ... 1 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh-mZevFyWRl",
        "outputId": "6c4ca0c7-2eb9-4479-cd34-23f4ae7ad2dd"
      },
      "source": [
        "review_data['Score'] = review_data['Score'].map(lambda x : 1 if int(x) > 3 else 0)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAiCNt8SzE-r",
        "outputId": "a1aa7c2d-91a8-44c1-a669-6d559f214604"
      },
      "source": [
        "review_data['Score'].value_counts()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    2297\n",
              "0     702\n",
              "Name: Score, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uvQKPr5z3kQ"
      },
      "source": [
        "def data_clean(text):\n",
        "    \n",
        "    ## Remove puncuation\n",
        "    text = text.translate(string.punctuation)\n",
        "    \n",
        "    ## Convert words to lower case and split them\n",
        "    text = text.lower().split()\n",
        "    \n",
        "    ## Remove stop words\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
        "    \n",
        "    text = \" \".join(text)\n",
        "\n",
        "    # Clean the text\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    # text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    # text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "    \n",
        "    text = text.split()\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    stemmed_words = [stemmer.stem(word) for word in text]\n",
        "    text = \" \".join(stemmed_words)\n",
        "\n",
        "    return text\n",
        "# def data_clean(text):\n",
        "#   #convert text to lower case\n",
        "#   text = text.lower()\n",
        "\n",
        "#   #remove numbers \n",
        "#   text = re.sub(r'\\d+', '', text)\n",
        "#   #remove punctuation\n",
        "#   text = text.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
        "\n",
        "#   #remove white spaces\n",
        "#   text = text.strip()\n",
        "\n",
        "#   #stop words removal\n",
        "#   stop_words = set(stopwords.words('english'))\n",
        "#   tokens = word_tokenize(text)\n",
        "#   result = [i for i in tokens if not i in stop_words]\n",
        "#   text = \" \".join(result)\n",
        "#   # print(text)\n",
        "\n",
        "#   text = text.split()\n",
        "#   stemmer = SnowballStemmer('english')\n",
        "#   stemmed_words = [stemmer.stem(word) for word in text]\n",
        "#   text = \" \".join(stemmed_words)\n",
        "\n",
        "#   #lemmatization\n",
        "#   lemmatizer=WordNetLemmatizer()\n",
        "#   text=word_tokenize(text)\n",
        "\n",
        "#   return text"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0adCr7uq0AQn"
      },
      "source": [
        "# clean_reviewData = []\n",
        "# for rev in review_data['Text']:\n",
        "#     clean_reviewData.append( \" \".join(data_clean(rev)))\n",
        "\n",
        "# clean_summaryData = []\n",
        "# for rev in review_data['Summary']:\n",
        "#     clean_summaryData.append( \" \".join(data_clean(rev)))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bsox7fRkikw"
      },
      "source": [
        "def data_clean(rev, remove_stopwords=True):\n",
        "  new_text = re.sub(\"[^a-zA-Z]\",\" \", rev)\n",
        "  words = new_text.lower().split()\n",
        "  if remove_stopwords:\n",
        "        sts = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in sts]\n",
        "  ary=[]\n",
        "  eng_stemmer = english_stemmer \n",
        "  for word in words:\n",
        "      ary.append(eng_stemmer.stem(word))  #stem the words to retain the root/base word\n",
        "  return(ary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "s51ay5xJ0C2P",
        "outputId": "9ec046d2-444a-46b3-81d1-522cbf5f7f80"
      },
      "source": [
        "Data = review_data[['Summary','Score']]\n",
        "print(Data.head())\n",
        "Data_x= Data['Summary']\n",
        "Data_y= Data['Score']\n",
        "#calculate padding length\n",
        "MAX_LEN = 0\n",
        "for i in range(1,2999):\n",
        "  if(MAX_LEN < len(Data_x[i])):\n",
        "    MAX_LEN = len(Data_x[i])\n",
        "print(MAX_LEN)\n",
        "pos=[]\n",
        "neg=[]\n",
        "for l in Data.Score:\n",
        "  if l==0:\n",
        "    pos.append(0)\n",
        "    neg.append(1)\n",
        "  elif l==1:\n",
        "    pos.append(1)\n",
        "    neg.append(0)\n",
        "\n",
        "Data['Pos']=pos\n",
        "Data['Neg']=neg\n",
        "\n",
        "Data = Data[['Summary', 'Score', 'Pos', 'Neg']]\n",
        "Data.head()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                 Summary  Score\n",
            "0  Good Quality Dog Food      1\n",
            "1      Not as Advertised      0\n",
            "2  \"Delight\" says it all      1\n",
            "3         Cough Medicine      0\n",
            "4            Great taffy      1\n",
            "126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Summary</th>\n",
              "      <th>Score</th>\n",
              "      <th>Pos</th>\n",
              "      <th>Neg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Cough Medicine</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Great taffy</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Summary  Score  Pos  Neg\n",
              "0  Good Quality Dog Food      1    1    0\n",
              "1      Not as Advertised      0    0    1\n",
              "2  \"Delight\" says it all      1    1    0\n",
              "3         Cough Medicine      0    0    1\n",
              "4            Great taffy      1    1    0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFQBGyuKkmP_"
      },
      "source": [
        "Data['Text_clean'] = Data['Summary'].apply(lambda x: data_clean(x))\n",
        "Data['Text_final'] = [' '.join(sen) for sen in Data['Text_clean']]\n",
        "Data['Tokens'] = Data['Text_clean']"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21cefDdYlNRK",
        "outputId": "4152e375-63af-46e2-cbd2-2679ec488654"
      },
      "source": [
        "Data = Data[['Text_final', 'Tokens', 'Score', 'Pos', 'Neg']]\n",
        "print(Data.head())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                  Text_final                 Tokens  ...  Pos  Neg\n",
            "0  g o o d   q u a l i t i   d o g   f o o d  good qualiti dog food  ...    1    0\n",
            "1                            a d v e r t i s               advertis  ...    0    1\n",
            "2                      d e l i g h t   s a y            delight say  ...    1    0\n",
            "3                  c o u g h   m e d i c i n          cough medicin  ...    0    1\n",
            "4                      g r e a t   t a f f i            great taffi  ...    1    0\n",
            "\n",
            "[5 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3I0gWBVjqUY"
      },
      "source": [
        "data_train, data_test = train_test_split(Data, test_size=0.1, random_state=42)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnGMfTCKmEFc",
        "outputId": "76521cf5-3f52-4d0c-9064-350a06cb2d16"
      },
      "source": [
        "all_training_words = [word for tokens in data_train['Tokens'] for word in tokens]\n",
        "training_sentence_lengths = [len(tokens) for tokens in data_train['Tokens']]\n",
        "Training_vocab = sorted(list(set(all_training_words)))\n",
        "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(Training_vocab)))\n",
        "print(\"Max sentence length is %s\" %max(training_sentence_lengths))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45910 words total, with a vocabulary size of 43\n",
            "Max sentence length is 83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROZrpvt1mPdq",
        "outputId": "10d75f3a-5618-48c3-c33a-c23e89dab285"
      },
      "source": [
        "all_test_words = [word for tokens in data_test['Tokens'] for word in tokens]\n",
        "test_sentence_lengths = [len(tokens) for tokens in data_test['Tokens']]\n",
        "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
        "print('%s words total, with a vocabulary size of %s' % (len(all_test_words), len(TEST_VOCAB)))\n",
        "print('Max sentence length is %s' % max(test_sentence_lengths))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5154 words total, with a vocabulary size of 35\n",
            "Max sentence length is 64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXBy4aWfml7X"
      },
      "source": [
        "MAX_LEN = 83"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNdLtpwM0Frv",
        "outputId": "5b578038-b3f1-4e6d-f7d3-381f925f1ec7"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=len(Training_vocab), lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(data_train[\"Text_final\"].tolist())\n",
        "training_sequences = tokenizer.texts_to_sequences(data_train[\"Text_final\"].tolist())\n",
        "\n",
        "train_word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(train_word_index))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 36 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzltF-g8nElU",
        "outputId": "1adc2074-4c64-4ed6-bf14-db48ae032179"
      },
      "source": [
        "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_LEN)\n",
        "print(train_cnn_data[0])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  8  4 19  1  2 14  1 16]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eql5fnhz3z1J",
        "outputId": "eff00a3f-d5ce-4c5d-d679-065ba7af5afb"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-23 15:01:04--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-11-23 15:01:04--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-11-23 15:01:04--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.10MB/s    in 6m 26s  \n",
            "\n",
            "2020-11-23 15:07:31 (2.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaJ_hiOx34pw",
        "outputId": "4f4d4b37-2118-40c2-c9d0-d81a7aa496e8"
      },
      "source": [
        "!unzip glove*.zip"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y53F-T1_0Vde",
        "outputId": "670bd92f-ae22-4a47-e21b-7c79b065a969"
      },
      "source": [
        "#using glove model for word embeddings\n",
        "embeddings_index = dict()\n",
        "f = open('glove.6B.300d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cg1k-g-Z1M2i"
      },
      "source": [
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = np.zeros((len(Training_vocab), 300))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    if index > vocabulary_size - 1:\n",
        "        break\n",
        "    else:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[index] = embedding_vector\n",
        "\n",
        "# print(embedding_matrix[1])"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yh80AQhBrG4o"
      },
      "source": [
        "test_sequences = tokenizer.texts_to_sequences(data_test[\"Text_final\"].tolist())\n",
        "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_LEN)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrRbCrzN1O1q"
      },
      "source": [
        "#create the model with CNN 5 filters are used and with a hidden layer and non trainable embedding layer\n",
        "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
        "  embedding_layer = Embedding(num_words, embedding_dim, weights=[embeddings],\n",
        "                              input_length=max_sequence_length, trainable=False)\n",
        "  sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
        "  embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "  convs = []\n",
        "  filter_sizes = [2,3,4,5,6]\n",
        "\n",
        "  for filter_size in filter_sizes:\n",
        "    l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
        "    l_pool = GlobalMaxPooling1D()(l_conv)\n",
        "    convs.append(l_pool)\n",
        "  \n",
        "  l_merge = concatenate(convs, axis=1)\n",
        "  x = Dropout(0.1)(l_merge)\n",
        "  x = Dense(128, activation='relu')(x)\n",
        "  x = Dropout(0.2)(x)\n",
        "  preds = Dense(labels_index, activation='sigmoid')(x)\n",
        "\n",
        "  model = Model(sequence_input, preds)\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam',\n",
        "                metrics=['mse'])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LgjX7NwE2ci",
        "outputId": "5697ce06-12e5-4297-dc28-5827748ba3f7"
      },
      "source": [
        "label_names = ['Pos','Neg']\n",
        "y_train = data_train[label_names].values\n",
        "print(y_train)\n",
        "model = ConvNet(embedding_matrix, 50, len(Training_vocab), 300, len(list(label_names)))\n",
        "x_train = train_cnn_data"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0]\n",
            " [1 0]\n",
            " [0 1]\n",
            " ...\n",
            " [1 0]\n",
            " [1 0]\n",
            " [0 1]]\n",
            "Model: \"functional_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 50)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 50, 300)      12900       input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_10 (Conv1D)              (None, 49, 200)      120200      embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_11 (Conv1D)              (None, 48, 200)      180200      embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_12 (Conv1D)              (None, 47, 200)      240200      embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_13 (Conv1D)              (None, 46, 200)      300200      embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_14 (Conv1D)              (None, 45, 200)      360200      embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_10 (Global (None, 200)          0           conv1d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_11 (Global (None, 200)          0           conv1d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_12 (Global (None, 200)          0           conv1d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_13 (Global (None, 200)          0           conv1d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_14 (Global (None, 200)          0           conv1d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 1000)         0           global_max_pooling1d_10[0][0]    \n",
            "                                                                 global_max_pooling1d_11[0][0]    \n",
            "                                                                 global_max_pooling1d_12[0][0]    \n",
            "                                                                 global_max_pooling1d_13[0][0]    \n",
            "                                                                 global_max_pooling1d_14[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 1000)         0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 128)          128128      dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 128)          0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 2)            258         dropout_5[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,342,286\n",
            "Trainable params: 1,329,386\n",
            "Non-trainable params: 12,900\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnF5eqB0q2gs"
      },
      "source": [
        "num_epochs = 3\n",
        "batch_size = 34"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5ShIEr-GaBt",
        "outputId": "8b8347b5-1795-45f2-e392-7cab18ebe250"
      },
      "source": [
        "hist = model.fit(x_train, y_train, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "72/72 [==============================] - 1s 11ms/step - loss: 0.4446 - mse: 0.1442 - val_loss: 0.4759 - val_mse: 0.1533\n",
            "Epoch 2/3\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 0.4068 - mse: 0.1348 - val_loss: 0.4664 - val_mse: 0.1491\n",
            "Epoch 3/3\n",
            "72/72 [==============================] - 1s 9ms/step - loss: 0.3682 - mse: 0.1205 - val_loss: 0.4612 - val_mse: 0.1505\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQ5Ufkf1q_mj",
        "outputId": "787addca-072d-4c67-b7c1-cc786a2a5e6c"
      },
      "source": [
        "predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 50) for input Tensor(\"input_4:0\", shape=(None, 50), dtype=int32), but it was called on an input with incompatible shape (None, 83).\n",
            "1/1 [==============================] - 0s 7ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuMMEy_mrQcJ"
      },
      "source": [
        "labels = [1,0]\n",
        "prediction_labels=[]\n",
        "for p in predictions:\n",
        "    prediction_labels.append(labels[np.argmax(p)])"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALnqHWLErTaX",
        "outputId": "621b1acb-8248-48b3-deba-230f3716f381"
      },
      "source": [
        "sum(data_test.Score==prediction_labels)/len(prediction_labels)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7866666666666666"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIQpE64HKQt7"
      },
      "source": [
        "sentence = \"worst product ever\"\n",
        "clean_reviewData = []\n",
        "clean_reviewData.append( \" \".join(data_clean(sentence)))"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3LxIZd0PBLD",
        "outputId": "81ad9ada-c876-4df5-a36d-fb047fcd6997"
      },
      "source": [
        "print(clean_reviewData)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['b e s t   p r o d u c t   e v e r']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ttedvyq-PDJN",
        "outputId": "1fc6c06c-13c0-4431-fd7e-47607c068b84"
      },
      "source": [
        "vocabulary_size = len(Training_vocab)\n",
        "tokenizer = Tokenizer(num_words=len(Training_vocab), lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(clean_reviewData)\n",
        "training_sequences = tokenizer.texts_to_sequences(clean_reviewData)\n",
        "training_sequences = pad_sequences(training_sequences, maxlen=MAX_LEN)\n",
        "print(training_sequences)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  4  1  5  2\n",
            "   6  3  7  8  9 10  2  1 11  1  3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZdCdiJm-jkj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3699482-31e6-4914-e85d-3d74d3830f32"
      },
      "source": [
        "output = model.predict(training_sequences)\n",
        "# print(output)\n",
        "print(label_names[np.argmax(output)])\n",
        "Accuracy = max(output[0])*100\n",
        "print(\"Accuracy: %.2f per cent\"%Accuracy)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Neg\n",
            "Accuracy: 51.61 per cent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLAoqBjU_69x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}