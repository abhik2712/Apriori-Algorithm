{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Analysis_CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWwYOJSsekNW",
        "outputId": "e6e1a2de-0fba-4cc4-f645-cc4729fdbc9c"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "english_stemmer=nltk.stem.SnowballStemmer('english')\n",
        "from nltk import word_tokenize\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim import models\n",
        "from keras.models import Model"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMvqEzMSeBPh",
        "outputId": "7dbd3d34-21a6-49e5-ecdd-eeb8889f6d9c"
      },
      "source": [
        "data = pd.read_csv('imdb_labelled.tsv', \n",
        "                   header = None, \n",
        "                   delimiter='\\t')\n",
        "data.columns = ['Text', 'Label']\n",
        "data.head()\n",
        "print(data.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(748, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC129z26eWFf",
        "outputId": "2fc7bb3f-6e30-4df6-e3ba-afafb2be87aa"
      },
      "source": [
        "data.Label.value_counts()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    386\n",
              "0    362\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "161jLMELefr-"
      },
      "source": [
        "def data_clean(rev, remove_stopwords=True):\n",
        "  new_text = re.sub(\"[^a-zA-Z]\",\" \", rev)\n",
        "  words = new_text.lower().split()\n",
        "  if remove_stopwords:\n",
        "        sts = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in sts]\n",
        "  ary=[]\n",
        "  eng_stemmer = english_stemmer \n",
        "  for word in words:\n",
        "      ary.append(eng_stemmer.stem(word))  #stem the words to retain the root/base word\n",
        "  return(ary)\n",
        "\n",
        "data['Text_Clean'] = data['Text'].apply(lambda x: data_clean(x))\n",
        "data['Text_Final'] = [' '.join(sen) for sen in data['Text_Clean']]\n",
        "data['tokens'] = data['Text_Clean']"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "6iribht6e8Xb",
        "outputId": "8c2bea9c-b7fe-424c-dcce-2ae822f31da4"
      },
      "source": [
        "pos=[]\n",
        "neg=[]\n",
        "for l in data.Label:\n",
        "  if l==0:\n",
        "    pos.append(0)\n",
        "    neg.append(1)\n",
        "  elif l==1:\n",
        "    pos.append(1)\n",
        "    neg.append(0)\n",
        "\n",
        "data['Pos']=pos\n",
        "data['Neg']=neg\n",
        "\n",
        "data = data[['Text_Final', 'tokens', 'Label', 'Pos', 'Neg']]\n",
        "data.head()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text_Final</th>\n",
              "      <th>tokens</th>\n",
              "      <th>Label</th>\n",
              "      <th>Pos</th>\n",
              "      <th>Neg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>slow move aimless movi distress drift young man</td>\n",
              "      <td>[slow, move, aimless, movi, distress, drift, y...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sure lost flat charact audienc near half walk</td>\n",
              "      <td>[sure, lost, flat, charact, audienc, near, hal...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>attempt arti black white clever camera angl mo...</td>\n",
              "      <td>[attempt, arti, black, white, clever, camera, ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>littl music anyth speak</td>\n",
              "      <td>[littl, music, anyth, speak]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>best scene movi gerardo tri find song keep run...</td>\n",
              "      <td>[best, scene, movi, gerardo, tri, find, song, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          Text_Final  ... Neg\n",
              "0    slow move aimless movi distress drift young man  ...   1\n",
              "1      sure lost flat charact audienc near half walk  ...   1\n",
              "2  attempt arti black white clever camera angl mo...  ...   1\n",
              "3                            littl music anyth speak  ...   1\n",
              "4  best scene movi gerardo tri find song keep run...  ...   0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izOhGmMygZhZ"
      },
      "source": [
        "data_train, data_test = train_test_split(data, test_size=0.10, random_state=42)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-Lv9z2FjNjD",
        "outputId": "9765c519-0d5c-4b3a-c699-249f493ca0a4"
      },
      "source": [
        "all_training_words = [word for tokens in data_train['tokens'] for word in tokens]\n",
        "training_sentence_lengths = [len(tokens) for tokens in data_train['tokens']]\n",
        "Training_vocab = sorted(list(set(all_training_words)))\n",
        "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(Training_vocab)))\n",
        "print(\"Max sentence length is %s\" %max(training_sentence_lengths))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6827 words total, with a vocabulary size of 2244\n",
            "Max sentence length is 677\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQSbanE9kLgM",
        "outputId": "64ac1743-c55c-4e76-f5e2-413eb6bc0a0e"
      },
      "source": [
        "all_test_words = [word for tokens in data_test['tokens'] for word in tokens]\n",
        "test_sentence_lengths = [len(tokens) for tokens in data_test['tokens']]\n",
        "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
        "print('%s words total, with a vocabulary size of %s' % (len(all_test_words), len(TEST_VOCAB)))\n",
        "print('Max sentence length is %s' % max(test_sentence_lengths))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "567 words total, with a vocabulary size of 414\n",
            "Max sentence length is 24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjcWkSE-mGXv",
        "outputId": "a2427f56-09d6-4f56-b729-6872166de035"
      },
      "source": [
        "!pip3 install wget"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=827c941742ba810f7cb4c3fe7efce5e40f14192174490b981b038e1a24f3ddf4\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMHDE7-tmmWk",
        "outputId": "144c7564-9d9d-4115-8239-0e96852860cf"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "wv = api.load('word2vec-google-news-300')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[================================================--] 96.2% 1600.3/1662.8MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IV-_4HsplKzG"
      },
      "source": [
        "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
        "    if len(tokens_list)<1:\n",
        "        return np.zeros(k)\n",
        "    if generate_missing:\n",
        "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
        "    else:\n",
        "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
        "    length = len(vectorized)\n",
        "    summed = np.sum(vectorized, axis=0)\n",
        "    averaged = np.divide(summed, length)\n",
        "    return averaged\n",
        "\n",
        "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
        "    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
        "                                                                                generate_missing=generate_missing))\n",
        "    return list(embeddings)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2h5B26EoEJj"
      },
      "source": [
        "training_embeddings = get_word2vec_embeddings(wv, data_train, generate_missing=True)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZolNzoptorxv"
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 50\n",
        "EMBEDDING_DIM = 300"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHgolvMDo1_Q",
        "outputId": "4ebf83a8-bb2f-4648-e78b-0fbdfd3d11f5"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=len(Training_vocab), lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(data_train[\"Text_Final\"].tolist())\n",
        "training_sequences = tokenizer.texts_to_sequences(data_train[\"Text_Final\"].tolist())\n",
        "\n",
        "train_word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(train_word_index))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2244 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QcoVPr5o5KV"
      },
      "source": [
        "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTM91lJApItU",
        "outputId": "bbe49a7c-1ed6-4d5a-83fe-9b577c8f5a5f"
      },
      "source": [
        "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
        "for word,index in train_word_index.items():\n",
        "    train_embedding_weights[index,:] = wv[word] if word in wv else np.random.rand(EMBEDDING_DIM)\n",
        "print(train_embedding_weights.shape)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2245, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNifNX91pS_j"
      },
      "source": [
        "test_sequences = tokenizer.texts_to_sequences(data_test[\"Text_Final\"].tolist())\n",
        "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K10TocawqDeV"
      },
      "source": [
        "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
        "    \n",
        "    embedding_layer = Embedding(num_words,\n",
        "                            embedding_dim,\n",
        "                            weights=[embeddings],\n",
        "                            input_length=max_sequence_length,\n",
        "                            trainable=False)\n",
        "    \n",
        "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "    convs = []\n",
        "    filter_sizes = [2,3,4,5,6]\n",
        "\n",
        "    for filter_size in filter_sizes:\n",
        "        l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
        "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
        "        convs.append(l_pool)\n",
        "\n",
        "\n",
        "    l_merge = concatenate(convs, axis=1)\n",
        "\n",
        "    x = Dropout(0.1)(l_merge)  \n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(sequence_input, preds)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['acc'])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3hgJ0G8qGfl"
      },
      "source": [
        "label_names = ['Pos', 'Neg']"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bMWrMlCqI-8"
      },
      "source": [
        "y_train = data_train[label_names].values"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7Yh8JYpqLGM"
      },
      "source": [
        "x_train = train_cnn_data\n",
        "y_tr = y_train"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqYc6zmQqNKM",
        "outputId": "d5692810-996d-4c3c-aa2d-f57956c5b694"
      },
      "source": [
        "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
        "                len(list(label_names)))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 50)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 50, 300)      673500      input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_10 (Conv1D)              (None, 49, 200)      120200      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_11 (Conv1D)              (None, 48, 200)      180200      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_12 (Conv1D)              (None, 47, 200)      240200      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_13 (Conv1D)              (None, 46, 200)      300200      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_14 (Conv1D)              (None, 45, 200)      360200      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_10 (Global (None, 200)          0           conv1d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_11 (Global (None, 200)          0           conv1d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_12 (Global (None, 200)          0           conv1d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_13 (Global (None, 200)          0           conv1d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_14 (Global (None, 200)          0           conv1d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 1000)         0           global_max_pooling1d_10[0][0]    \n",
            "                                                                 global_max_pooling1d_11[0][0]    \n",
            "                                                                 global_max_pooling1d_12[0][0]    \n",
            "                                                                 global_max_pooling1d_13[0][0]    \n",
            "                                                                 global_max_pooling1d_14[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 1000)         0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 128)          128128      dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 128)          0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 2)            258         dropout_5[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 2,002,886\n",
            "Trainable params: 1,329,386\n",
            "Non-trainable params: 673,500\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFtlSIBtqPSE"
      },
      "source": [
        "num_epochs = 3\n",
        "batch_size = 34"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLtKGSzoqoBK",
        "outputId": "b9ab44ce-87e2-4c31-bc4d-d11f2965f821"
      },
      "source": [
        "hist = model.fit(x_train, y_tr, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "18/18 [==============================] - 3s 172ms/step - loss: 0.7399 - acc: 0.5686 - val_loss: 0.6669 - val_acc: 0.6029\n",
            "Epoch 2/3\n",
            "18/18 [==============================] - 3s 160ms/step - loss: 0.6102 - acc: 0.7074 - val_loss: 0.6204 - val_acc: 0.6912\n",
            "Epoch 3/3\n",
            "18/18 [==============================] - 3s 161ms/step - loss: 0.4921 - acc: 0.7785 - val_loss: 0.5833 - val_acc: 0.6912\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKZ1J5qQq2lf",
        "outputId": "1926ce8e-8336-4781-d081-8b0d79c85d64"
      },
      "source": [
        "predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 2ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmVqsp6gqp5I"
      },
      "source": [
        "labels = [1,0]\n",
        "prediction_labels=[]\n",
        "for p in predictions:\n",
        "    prediction_labels.append(labels[np.argmax(p)])"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sR50wNbqzvJ",
        "outputId": "fb63d430-99b4-457b-d295-ca9e75caff64"
      },
      "source": [
        "sum(data_test.Label==prediction_labels)/len(prediction_labels)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7466666666666667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qImyVDPq7-c",
        "outputId": "6149d0f6-38d6-4186-be59-93c481995b87"
      },
      "source": [
        "data_test.Label.value_counts()"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    44\n",
              "1    31\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tkz94ZSCq-Lx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}